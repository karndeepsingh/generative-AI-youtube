{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DEPENDENCIES INSTALL"
      ],
      "metadata": {
        "id": "nSCE4XJkdEcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install langchain"
      ],
      "metadata": {
        "id": "TOyVaq6r3oVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84579ef7-5dd9-4dbd-a1c2-27601d6a4430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "ca6fd4f1-b0ac-49fb-dc1e-84e1b7b07ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  3 14:05:53 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA2-7B-Chat\n"
      ],
      "metadata": {
        "id": "H0shki19igLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\",\n",
        "                                          use_auth_token=True,)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                             load_in_8bit=True,\n",
        "                                            #  load_in_4bit=True\n",
        "                                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "5f6b40a2b0494fa69125b38586080706",
            "87dedd8fdb41455b9be28fb8efc13d08",
            "4df34d26a12145819381e76c8f62ad4d",
            "e13f1a0a6787489dbea8297ef3f47409",
            "5bdc37e091af4aa19887442f19777354",
            "3403d594f1384e6093a4698b70c2246f",
            "b196f5d6fa8d4ca5baae3dd3638188ec",
            "2dbc7a2434de4590a4e6b01625b5ed9a",
            "4eb41b8675c444a2b6be1fac47013f1e",
            "bd5a995904a443b4b922cad753378db5",
            "2124f7a281cd4ad3b39470a43ae0ff8f"
          ]
        },
        "id": "B4Xq1at9iixB",
        "outputId": "b89ca519-b545-4444-9d1a-9362860bb589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f6b40a2b0494fa69125b38586080706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline for later\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,\n",
        "                do_sample=True,\n",
        "                top_k=30,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )"
      ],
      "metadata": {
        "id": "fvuuA6wF1JH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8IS_kfwtE1uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5a041f-b97e-4499-d821-8cba31ff4bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  3 14:07:48 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    27W /  70W |   8097MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **PROMPT FORMAT FOR LLAMA-2**\n",
        "[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "{prompt} [/INST]\n"
      ],
      "metadata": {
        "id": "v3N9iwILeSpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo-FSysZiVkA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "\n",
        "def generate(text):\n",
        "    prompt = get_prompt(text)\n",
        "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        outputs = model.generate(**inputs,\n",
        "                                 max_new_tokens=512,\n",
        "                                 eos_token_id=tokenizer.eos_token_id,\n",
        "                                 pad_token_id=tokenizer.eos_token_id,\n",
        "                                 )\n",
        "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
        "        final_outputs = remove_substring(final_outputs, prompt)\n",
        "\n",
        "    return final_outputs#, outputs\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"How to make ISRO is successful in making Satellites and Lunar Missions?\"\n",
        "\n",
        "get_prompt(instruction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "oTf-vPB8wPGs",
        "outputId": "66bf411f-1560-4c59-c366-93fb437bb354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nHow to make ISRO is successful in making Satellites and Lunar Missions?[/INST]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following text : {text}\"\n",
        "\n",
        "system_prompt = \"You are an expert and summarization and reducing the number of words used\"\n",
        "\n",
        "get_prompt(instruction, system_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "R7n4grsO1UN1",
        "outputId": "f41c294a-3c42-4904-ab1b-1e5619d7f60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[INST]<<SYS>>\\nYou are an expert and summarization and reducing the number of words used\\n<</SYS>>\\n\\nSummarize the following text : {text}[/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain"
      ],
      "metadata": {
        "id": "-fTKdQRCdis7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate,  LLMChain\n"
      ],
      "metadata": {
        "id": "SP4Bk5YBf1mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "LL7JGQ5iCzIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation"
      ],
      "metadata": {
        "id": "IIOiEea5d8Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are an advanced assistant and transalation system \"\n",
        "instruction = \"Convert the following text from English to Spanish:\\n\\n {text}\"\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTBrt6XmuY3Y",
        "outputId": "9faf666e-6b3b-4338-c6af-58b0185e88c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are an advanced assistant and transalation system \n",
            "<</SYS>>\n",
            "\n",
            "Convert the following text from English to Spanish:\n",
            "\n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How to make chatbot in easiest way possible?\"\n",
        "output = llm_chain.run(text)\n",
        "\n",
        "parse_text(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0fUGp-muY6f",
        "outputId": "582c43f9-665e-4903-c77a-3e1b17415bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ¡Claro! Here's the translation of \"How to make chatbot in easiest way possible?\" from English to\n",
            "Spanish:  ¿Cómo crear chatbot de la manera más fácil posible?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization"
      ],
      "metadata": {
        "id": "e38eSqFb3hET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following article : {text}\"\n",
        "system_prompt = \"You are an expert in summarization of articles and expressing the important information\"\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt4mn5CJuY90",
        "outputId": "4da41004-e378-4e2c-9d6c-5e0e5b5a163d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are an expert in summarization of articles and expressing the important information\n",
            "<</SYS>>\n",
            "\n",
            "Summarize the following article : {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(input_string):\n",
        "    words = input_string.split(\" \")\n",
        "    return len(words)\n",
        "\n",
        "text = '''\n",
        "Chandrayaan-3 consists of an indigenous Lander module (LM), Propulsion module (PM) and a Rover with an objective of developing and demonstrating new technologies required for Inter planetary missions. The Lander will have the capability to soft land at a specified lunar site and deploy the Rover which will carry out in-situ chemical analysis of the lunar surface during the course of its mobility. The Lander and the Rover have scientific payloads to carry out experiments on the lunar surface. The main function of PM is to carry the LM from launch vehicle injection till final lunar 100 km circular polar orbit and separate the LM from PM. Apart from this, the Propulsion Module also has one scientific payload as a value addition which will be operated post separation of Lander Module. The launcher identified for Chandrayaan-3 is LVM3 M4 which will place the integrated module in an Elliptic Parking Orbit (EPO) of size ~170 x 36500 km.\n",
        "\n",
        "The mission objectives of Chandrayaan-3 are:\n",
        "\n",
        "To demonstrate Safe and Soft Landing on Lunar Surface\n",
        "To demonstrate Rover roving on the moon and\n",
        "To conduct in-situ scientific experiments.\n",
        "To achieve the mission objectives, several advanced technologies are present in Lander such as,\n",
        "\n",
        "Altimeters: Laser & RF based Altimeters\n",
        "Velocimeters: Laser Doppler Velocimeter & Lander Horizontal Velocity Camera\n",
        "Inertial Measurement: Laser Gyro based Inertial referencing and Accelerometer package\n",
        "Propulsion System: 800N Throttleable Liquid Engines, 58N attitude thrusters & Throttleable Engine Control Electronics\n",
        "Navigation, Guidance & Control (NGC): Powered Descent Trajectory design and associate software elements\n",
        "Hazard Detection and Avoidance: Lander Hazard Detection & Avoidance Camera and Processing Algorithm\n",
        "Landing Leg Mechanism.\n",
        "To demonstrate the above said advanced technologies in earth condition, several Lander special tests have been planned and carried out successfully viz.\n",
        "\n",
        "Integrated Cold Test - For the demonstration of Integrated Sensors & Navigation performance test using helicopter as test platform\n",
        "Integrated Hot test – For the demonstration of closed loop performance test with sensors, actuators and NGC using Tower crane as test platform\n",
        "Lander Leg mechanism performance test on a lunar simulant test bed simulating different touch down conditions.'''\n",
        "\n",
        "count_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gykQOf4OuZBK",
        "outputId": "ffe1ff03-102a-42e3-9794-aa30883cd39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "331"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_chain.run(text)\n",
        "print(count_words(output))\n",
        "parse_text(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4VDYh3VuZD_",
        "outputId": "4e1b8cdb-4a5e-4649-c030-af706c565be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "275\n",
            "  Chandrayaan-3 is an Indian space mission aimed at demonstrating new technologies for\n",
            "interplanetary missions. The mission consists of a lander module (LM), propulsion module (PM), and a\n",
            "rover, all of which are equipped with scientific payloads to conduct experiments on the lunar\n",
            "surface. The main function of the PM is to carry the LM from launch to final lunar orbit and\n",
            "separate it from the PM. The PM also has a scientific payload for post-separation operation. The\n",
            "launch vehicle identified for Chandrayaan-3 is the LVM3 M4, which will place the integrated module\n",
            "in an elliptic parking orbit (EPO) of size ~170 x 36500 km.  The mission objectives of Chandrayaan-3\n",
            "are:  1. Safe and soft landing on the lunar surface 2. Rover roving on the moon 3. Conducting in-\n",
            "situ scientific experiments  To achieve these objectives, several advanced technologies are employed\n",
            "in the Lander, including:  1. Altimeters (Laser & RF based) 2. Velocimeters (Laser Doppler\n",
            "Velocimeter & Lander Horizontal Velocity Camera) 3. Inertial Measurement (Laser Gyro based Inertial\n",
            "referencing and Accelerometer package) 4. Propulsion System (800N Throttleable Liquid Engines, 58N\n",
            "attitude thrusters & Throttleable Engine Control Electronics) 5. Navigation, Guidance & Control\n",
            "(NGC) (Powered Descent Trajectory design and associate software elements) 6. Hazard Detection and\n",
            "Avoidance (Lander Hazard Detection & Avoidance Camera and Processing Algorithm) 7. Landing Leg\n",
            "Mechanism  To demonstrate these advanced technologies in Earth conditions, several special tests\n",
            "have been conducted successfully, including:  1. Integrated Cold Test - Demonstration of integrated\n",
            "sensors & navigation performance using a helicopter as a test platform 2. Integrated Hot Test -\n",
            "Closed-loop performance test with sensors, actuators, and NGC using a tower crane as a test platform\n",
            "3. Lander Leg mechanism performance test on a lunar simulant test bed simulating different touchdown\n",
            "conditions.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Chatbot"
      ],
      "metadata": {
        "id": "MsPFAOBK74Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate"
      ],
      "metadata": {
        "id": "Inghmejz_Tkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}\"\n",
        "system_prompt = \"You are a helpful chat assistant.You always read the chat and respond and then stop. Read the chat history to get context\"\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7DAJoLEAOU7",
        "outputId": "b45c93d6-a81d-4294-ebf1-01079f035faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "You are a helpful chat assistant.You always read the chat and respond and then stop. Read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "{chat_history} \n",
            "\n",
            "User: {user_input}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"user_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "Qv7iPmbI_TnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "CGjjiqxz_TqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Hi, I am Karndeep. How are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "X3VadjbM_Tsx",
        "outputId": "406c65be-17d7-4956-9c40-5f758c8d9438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful chat assistant.You always read the chat and respond and then stop. Read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            " \n",
            "\n",
            "User: Hi, I am Karndeep. How are you?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Hello Karndeep! I'm doing well, thank you for asking! It's great to chat with you today. How can I assist you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"How you are helpful in assisting me?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "ENqnCnun_TvC",
        "outputId": "ad9f2dd8-c9e4-4df3-bea6-12e205aeb7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful chat assistant.You always read the chat and respond and then stop. Read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, I am Karndeep. How are you?\n",
            "AI:   Hello Karndeep! I'm doing well, thank you for asking! It's great to chat with you today. How can I assist you? \n",
            "\n",
            "User: How you are helpful in assisting me?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  AI: I'd be happy to help you in any way I can! Is there something specific you need assistance with or a task you'd like me to help you complete? Please let me know and I'll do my best to assist you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(user_input=\"Can you help me to write step by step approach to build a successful online business?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "_HIqTiLc_Tx2",
        "outputId": "745f4606-cbd9-4dc5-919a-18e10a59ac43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
            "You are a helpful chat assistant.You always read the chat and respond and then stop. Read the chat history to get context\n",
            "<</SYS>>\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Hi, I am Karndeep. How are you?\n",
            "AI:   Hello Karndeep! I'm doing well, thank you for asking! It's great to chat with you today. How can I assist you?\n",
            "Human: How you are helpful in assisting me?\n",
            "AI:   AI: I'd be happy to help you in any way I can! Is there something specific you need assistance with or a task you'd like me to help you complete? Please let me know and I'll do my best to assist you. \n",
            "\n",
            "User: Can you help me to write step by step approach to build a successful online business?[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  AI: Of course, I'd be happy to help you with that! Building a successful online business requires a well-thought-out approach that covers various aspects. Here's a step-by-step approach to help you get started:\\n\\nStep 1: Define Your Niche\\n\\n* Identify your area of expertise and passion.\\n* Research your target audience and their needs.\\n* Choose a niche that aligns with your expertise and has a sufficient audience.\\n\\nStep 2: Set Clear Goals\\n\\n* Define your business goals and objectives.\\n* Determine your target audience and their needs.\\n* Set specific, measurable, achievable, relevant, and time-bound (SMART) goals.\\n\\nStep 3: Conduct Market Research\\n\\n* Analyze your competition and their strategies.\\n* Identify potential customers and their buying habits.\\n* Determine the demand for your products or services in the market.\\n\\nStep 4: Create a Business Plan\\n\\n* Develop a detailed business plan that outlines your goals, strategies, and tactics.\\n* Define your business model and revenue streams.\\n* Identify potential challenges and develop contingency plans.\\n\\nStep 5: Build a Website or Online Platform\\n\\n* Create a professional-looking website or online platform that showcases your products or services.\\n* Make sure your website is user-friendly and mobile-friendly.\\n* Include calls-to-action to encourage visitors to take action.\\n\\nStep 6: Develop Engaging Content\\n\\n* Create high-quality, engaging content that adds value to your visitors.\\n* Use various content formats such as blog posts, videos, podcasts, and social media posts.\\n* Develop a content calendar to stay organized and consistent.\\n\\nStep 7: Optimize for SEO\\n\\n* Conduct keyword research to identify relevant keywords for your business.\\n* Optimize your website and content for SEO.\\n* Build backlinks from high-quality, relevant websites.\\n\\nStep 8: Launch and Promote Your Business\\n\\n* Launch your website or online platform.\\n* Develop a marketing plan that includes social media, email marketing, and paid advertising.\\n* Promote your business through various channels to reach your target audience.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4aLRBjEBOW9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f6b40a2b0494fa69125b38586080706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87dedd8fdb41455b9be28fb8efc13d08",
              "IPY_MODEL_4df34d26a12145819381e76c8f62ad4d",
              "IPY_MODEL_e13f1a0a6787489dbea8297ef3f47409"
            ],
            "layout": "IPY_MODEL_5bdc37e091af4aa19887442f19777354"
          }
        },
        "87dedd8fdb41455b9be28fb8efc13d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3403d594f1384e6093a4698b70c2246f",
            "placeholder": "​",
            "style": "IPY_MODEL_b196f5d6fa8d4ca5baae3dd3638188ec",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4df34d26a12145819381e76c8f62ad4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dbc7a2434de4590a4e6b01625b5ed9a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4eb41b8675c444a2b6be1fac47013f1e",
            "value": 2
          }
        },
        "e13f1a0a6787489dbea8297ef3f47409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd5a995904a443b4b922cad753378db5",
            "placeholder": "​",
            "style": "IPY_MODEL_2124f7a281cd4ad3b39470a43ae0ff8f",
            "value": " 2/2 [01:01&lt;00:00, 28.03s/it]"
          }
        },
        "5bdc37e091af4aa19887442f19777354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3403d594f1384e6093a4698b70c2246f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b196f5d6fa8d4ca5baae3dd3638188ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dbc7a2434de4590a4e6b01625b5ed9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb41b8675c444a2b6be1fac47013f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd5a995904a443b4b922cad753378db5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2124f7a281cd4ad3b39470a43ae0ff8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}